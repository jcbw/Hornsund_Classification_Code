{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661bf160-9c37-4dac-beb5-e75cb109348b",
   "metadata": {},
   "source": [
    "# <u>Geometric image classification substructure of the Hornsund Fjord: Mechanical Engineering applications in remote sensing and Geography Information Systems (GIS) (For Loop Version) </u>  <br /> <sub> <span style=\"font-size:smaller;\"> Jullian C.B. Williams </span> </sub> <br /> <sub> <span style=\"font-size:smaller;\"> *Polish Academy of Sciences, Institute of Geophysics, Warsaw, Poland* </span> </sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d605af-6e21-4476-85e8-52484840a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shapely\n",
    "# !pip install geopandas\n",
    "# !pip install fiona\n",
    "# !pip install rasterstats\n",
    "# !pip install spectral\n",
    "# !pip install pyarrow\n",
    "# !pip install chardet\n",
    "# !pip install fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3beb26-c179-4529-b235-024472ded431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji #<---- Death and taxes are for certain. Try to have fun before you're 6 feet too far under to.\n",
    "from rich.console import Console #<---- Ditto.\n",
    "\n",
    "import fiona\n",
    "import random\n",
    "import warnings\n",
    "import argparse, os\n",
    "import pyarrow as pa\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.affinity import translate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import ogr\n",
    "from osgeo import gdal #<---- to geospatial env.\n",
    "import geopandas as gpd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist \n",
    "from rasterstats import zonal_stats\n",
    "from rasterio.features import shapes\n",
    "from rasterio.plot import adjust_band\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.image\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from spectral import *\n",
    "from sklearn import svm\n",
    "from scipy import stats\n",
    "from scipy import ndimage\n",
    "from sklearn.svm import SVC\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.ndimage import sobel\n",
    "from scipy.stats import entropy\n",
    "import spectral.io.envi as envi #run PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from skimage import io, transform # from scipy.misc import imresize  <---- here...we...go..\n",
    "from skimage.morphology import disk #manage disk while running.\n",
    "from skimage.segmentation import slic\n",
    "from skimage.util import img_as_ubyte #Avoid precision loss converting image of type float32 to uint8 as required by rank filters.\n",
    "from skimage.filters.rank import entropy #for GLCM entropy.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skimage.segmentation import chan_vese\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import graycomatrix, graycoprops # from skimage.feature import greycomatrix, greycoprops <--- depreciated or..? lang.\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e607a-a5b2-4e94-bed4-9b7a5518c23f",
   "metadata": {},
   "source": [
    "## Loop SVM raster to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd29f0-60f0-412e-9f31-68751cdd5aa2",
   "metadata": {},
   "source": [
    "#### Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114f84e9-8e4f-4163-82ee-8f44f317174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/S1/EW_GLCM_stack_cont/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/S1/EW_parquet_2/\"\n",
    "\n",
    "raster_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/output_bandsSVMtrain_Pair.tif\"\n",
    "with rio.open(raster_path) as src:\n",
    "    image = src.read()  # Read all bands\n",
    "    profile = src.profile  # Save metadata\n",
    "    b1 = src.read(1)\n",
    "    b2 = src.read(2)\n",
    "    b3 = src.read(3)\n",
    "    b4 = src.read(4)\n",
    "    b5 = src.read(5)\n",
    "    b6 = src.read(6)\n",
    "    b7 = src.read(7)\n",
    "    b8 = src.read(8)\n",
    "    b9 = src.read(9)\n",
    "    b10 = src.read(10)\n",
    "    b11 = src.read(11)\n",
    "    b12 = src.read(12)\n",
    "    b13 = src.read(13)\n",
    "    b14 = src.read(14)\n",
    "\n",
    "    image = np.moveaxis(image, 0, -1)  # Rearrange to (rows, cols, bands)\n",
    "\n",
    "    bands = np.dstack((b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,b11,b12,b13,b14))  # <-- Why must b1 be alpha? Odd.\n",
    "    bands = bands.reshape(int(np.prod(bands.shape)/14),14)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through all .tif files in the input directory\n",
    "    for subdir, _, files in os.walk(input_dir):\n",
    "        for filename in files: \n",
    "            # Check if the file is an image (e.g., .jpg, .png)\n",
    "            if filename.endswith(('.tif')):\n",
    "                input_path = os.path.join(subdir, filename)\n",
    "                output_path = os.path.join(output_dir, filename.replace(\".tif\", \".shp\"))\n",
    "                 \n",
    "            with rio.open(input_path) as src_:\n",
    "                profile_ = src_.profile  # Save metadata\n",
    "                transform_ = src_.transform\n",
    "                crs_=src_.crs\n",
    "                b1_ = src_.read(1)\n",
    "                b2_ = src_.read(2)\n",
    "                b3_ = src_.read(3)\n",
    "                b4_ = src_.read(4)\n",
    "                b5_ = src_.read(5)\n",
    "                b6_ = src_.read(6)\n",
    "                b7_ = src_.read(7)\n",
    "                b8_ = src_.read(8)\n",
    "                b9_ = src_.read(9)\n",
    "                b10_ = src_.read(10)\n",
    "                b11_ = src_.read(11)\n",
    "                b12_ = src_.read(12)\n",
    "                b13_ = src_.read(13)\n",
    "                b14_ = src_.read(14)\n",
    "            \n",
    "                b1_ = np.where(np.isinf(b1_), np.nan, b1_)\n",
    "                b2_ = np.where(np.isinf(b2_), np.nan, b2_)          \n",
    "                b3_ = np.where(np.isinf(b3_), np.nan, b3_)\n",
    "                b4_ = np.where(np.isinf(b4_), np.nan, b4_)\n",
    "                b5_ = np.where(np.isinf(b5_), np.nan, b5_)\n",
    "                b6_ = np.where(np.isinf(b6_), np.nan, b6_)\n",
    "                b7_ = np.where(np.isinf(b7_), np.nan, b7_)\n",
    "                b8_ = np.where(np.isinf(b8_), np.nan, b8_)\n",
    "                b9_ = np.where(np.isinf(b9_), np.nan, b9_)\n",
    "                b10_ = np.where(np.isinf(b10_), np.nan, b10_)\n",
    "                b11_ = np.where(np.isinf(b11_), np.nan, b11_)\n",
    "                b12_ = np.where(np.isinf(b12_), np.nan, b12_)\n",
    "                b13_ = np.where(np.isinf(b13_), np.nan, b13_)\n",
    "                b14_ = np.where(np.isinf(b14_), np.nan, b14_)\n",
    "                            \n",
    "                nan1 = 9\n",
    "                nan2 = 81\n",
    "                \n",
    "                b1_ = np.nan_to_num(b1_, nan=nan1)\n",
    "                b2_ = np.nan_to_num(b2_, nan=nan1)\n",
    "                b3_ = np.nan_to_num(b3_, nan=nan1)\n",
    "                b4_ = np.nan_to_num(b4_, nan=nan1)\n",
    "                b5_ = np.nan_to_num(b5_, nan=nan2)\n",
    "                b6_ = np.nan_to_num(b6_, nan=nan2)\n",
    "                b7_ = np.nan_to_num(b7_, nan=nan1)\n",
    "                b8_ = np.nan_to_num(b8_, nan=nan1)\n",
    "                b9_ = np.nan_to_num(b9_, nan=nan1)\n",
    "                b10_ = np.nan_to_num(b10_, nan=nan1)\n",
    "                b11_ = np.nan_to_num(b11_, nan=nan1)\n",
    "                b12_ = np.nan_to_num(b12_, nan=nan1)\n",
    "                b13_ = np.nan_to_num(b13_, nan=nan2)\n",
    "                b14_ = np.nan_to_num(b14_, nan=nan2)\n",
    "            \n",
    "            \n",
    "                mask1 = (b1_ ==9) & (b1_== 81)\n",
    "                mask2 = (b2_ ==9) & (b2_== 81)\n",
    "                mask3 = (b3_ ==9) & (b3_== 81)\n",
    "                mask4 = (b4_ ==9) & (b4_== 81)\n",
    "                mask5 = (b5_ ==9) & (b5_== 81)\n",
    "                mask6 = (b6_ ==9) & (b6_== 81)\n",
    "                mask7 = (b7_ ==9) & (b7_== 81)\n",
    "                mask8 = (b8_ ==9) & (b8_== 81)\n",
    "                mask9 = (b9_ ==9) & (b9_== 81)\n",
    "                mask10 = (b10_ ==9) & (b10_== 81)\n",
    "                mask11 = (b11_ ==9) & (b11_== 81)\n",
    "                mask12 = (b12_ ==9) & (b12_== 81)\n",
    "                mask13 = (b13_ ==9) & (b13_== 81)\n",
    "                mask14 = (b14_ ==9) & (b14_== 81)\n",
    "                \n",
    "                b1_ = np.ma.masked_array(b1_, mask1)\n",
    "                b2_ = np.ma.masked_array(b2_, mask2)\n",
    "                b3_ = np.ma.masked_array(b3_, mask3)\n",
    "                b4_ = np.ma.masked_array(b4_, mask4)\n",
    "                b5_ = np.ma.masked_array(b5_, mask5)\n",
    "                b6_ = np.ma.masked_array(b6_, mask6)\n",
    "                b7_ = np.ma.masked_array(b7_, mask7)\n",
    "                b8_ = np.ma.masked_array(b8_, mask8)\n",
    "                b9_ = np.ma.masked_array(b9_, mask9)\n",
    "                b10_ = np.ma.masked_array(b10_, mask10)\n",
    "                b11_ = np.ma.masked_array(b11_, mask11)\n",
    "                b12_ = np.ma.masked_array(b12_, mask12)\n",
    "                b13_ = np.ma.masked_array(b13_, mask13)\n",
    "                b14_ = np.ma.masked_array(b14_, mask14)\n",
    "            \n",
    "                # image_ = np.moveaxis(image_, 0, -1)  # Rearrange to (rows, cols, bands)\n",
    "            \n",
    "                bands_ = np.dstack((b1_,b2_,b3_,b4_,b5_,b6_,b7_,b8_,b9_,b10_,b11_,b12_,b13_,b14_))  # <-- Why must b1 be alpha? Odd.\n",
    "                bands_ = bands_.reshape(int(np.prod(bands_.shape)/14),14)\n",
    "            \n",
    "            \n",
    "            # Load the shapefile\n",
    "            shapefile_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/ROI/PTS_MERGED/HS_ice_water_MASTER_.shp\"\n",
    "            shapefile = gpd.read_file(shapefile_path)\n",
    "            \n",
    "            # Extract training data\n",
    "            # Assuming the shapefile has a 'class' column for labels\n",
    "            training_data = []\n",
    "            labels = []\n",
    "            \n",
    "            for _, row in shapefile.iterrows():\n",
    "                geom = row.geometry\n",
    "                label = row['Class_']\n",
    "                mask = rio.features.geometry_mask([geom], transform=profile['transform'], invert=True, out_shape=image.shape[:2])\n",
    "                pixels = image[mask]\n",
    "                training_data.append(pixels)\n",
    "                labels.extend([label] * len(pixels))\n",
    "            \n",
    "            training_data = np.vstack(training_data)\n",
    "            labels = np.array(labels)\n",
    "            \n",
    "            #Train the SVM model\n",
    "            X_train, X_test, y_train, y_test = train_test_split(training_data, labels, test_size=0.3, random_state=42)\n",
    "            clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-3))\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            #Classify the image\n",
    "            predicted = clf.predict(bands_) #Predict using the model\n",
    "            class_image = predicted.reshape(b1_.shape)\n",
    "            \n",
    "            def image_to_geodataframe(image_array, transform, crs):\n",
    "            # Generate shapes (polygons) from the raster array\n",
    "                shapes_generator = shapes(image_array, transform=transform_)\n",
    "                \n",
    "                # Create a GeoDataFrame\n",
    "                geometries = []\n",
    "                values = []\n",
    "                for geom, value in shapes_generator:\n",
    "                    geometries.append(shape(geom))\n",
    "                    values.append(value)\n",
    "                \n",
    "                gdf = gpd.GeoDataFrame({'value': values, 'geometry': geometries}, crs=crs_)\n",
    "                \n",
    "                return gdf \n",
    "                \n",
    "            class_gdf = image_to_geodataframe(class_image, transform=transform_, crs=crs_)\n",
    "            \n",
    "            class_gdf['area'] = class_gdf.geometry.area\n",
    "            class_gdf['perimeter'] = class_gdf.geometry.length\n",
    "            class_gdf['centroid'] = class_gdf.geometry.centroid\n",
    "            # Extract latitude and longitude from centroids\n",
    "            class_gdf['longitude'] = class_gdf.centroid.x\n",
    "            class_gdf['latitude'] = class_gdf.centroid.y\n",
    "            class_gdf.set_crs(epsg=32633, inplace=True)\n",
    "            # print(class_gdf)\n",
    "            \n",
    "            # Function to calculate orientation\n",
    "            def calculate_orientation(geometry):\n",
    "                coords = np.array(geometry.exterior.coords)\n",
    "                x = coords[:, 0]\n",
    "                y = coords[:, 1]\n",
    "                \n",
    "                # Covariance matrix\n",
    "                cov_matrix = np.cov(x, y)\n",
    "                \n",
    "                # Eigenvalues and eigenvectors\n",
    "                eigvals, eigvecs = np.linalg.eig(cov_matrix)\n",
    "                \n",
    "                # Orientation angle (radians to degrees)\n",
    "                largest_eigenvec = eigvecs[:, np.argmax(eigvals)]\n",
    "                angle = np.arctan2(largest_eigenvec[1], largest_eigenvec[0])\n",
    "                return np.degrees(angle)\n",
    "            \n",
    "            # Function to calculate roundness\n",
    "            roundness = (4 * np.pi * class_gdf['area']) / (class_gdf['perimeter'] ** 2)\n",
    "            \n",
    "            # Apply orientation calculation\n",
    "            class_gdf[\"orientation\"] = class_gdf.geometry.apply(calculate_orientation)\n",
    "            class_gdf[\"roundness\"] = roundness\n",
    "            class_gdf.set_crs(epsg=32633, inplace=True)\n",
    "            gdf = class_gdf\n",
    "            # print(gdf)\n",
    "            \n",
    "            gdf.to_parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb6e5c-0eb7-4ee1-a7f4-dff75574a644",
   "metadata": {},
   "source": [
    "#### Summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7793d967-618a-4315-bfcd-b685f33c5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/S1/EW_GLCM_stack/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/S1/EW_GLCM_stack/EW_Summer_parquet/\"\n",
    "\n",
    "raster_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/output_bandsSVMtrain_Pair.tif\"\n",
    "with rio.open(raster_path) as src:\n",
    "    image = src.read()  # Read all bands\n",
    "    profile = src.profile  # Save metadata\n",
    "    b1 = src.read(1)\n",
    "    b2 = src.read(2)\n",
    "    b3 = src.read(3)\n",
    "    b4 = src.read(4)\n",
    "    b5 = src.read(5)\n",
    "    b6 = src.read(6)\n",
    "    b7 = src.read(7)\n",
    "    b8 = src.read(8)\n",
    "    b9 = src.read(9)\n",
    "    b10 = src.read(10)\n",
    "    b11 = src.read(11)\n",
    "    b12 = src.read(12)\n",
    "    b13 = src.read(13)\n",
    "    b14 = src.read(14)\n",
    "\n",
    "    image = np.moveaxis(image, 0, -1)  # Rearrange to (rows, cols, bands)\n",
    "\n",
    "    bands = np.dstack((b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,b11,b12,b13,b14))  # <-- Why must b1 be alpha? Odd.\n",
    "    bands = bands.reshape(int(np.prod(bands.shape)/14),14)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through all .tif files in the input directory\n",
    "    for subdir, _, files in os.walk(input_dir):\n",
    "        for filename in files: \n",
    "            # Check if the file is an image (e.g., .jpg, .png)\n",
    "            if filename.endswith(('.tif')):\n",
    "                input_path = os.path.join(subdir, filename)\n",
    "                output_path = os.path.join(output_dir, filename.replace(\".tif\", \".shp\"))\n",
    "                 \n",
    "            with rio.open(input_path) as src_:\n",
    "                profile_ = src_.profile  # Save metadata\n",
    "                transform_ = src_.transform\n",
    "                crs_=src_.crs\n",
    "                b1_ = src_.read(1)\n",
    "                b2_ = src_.read(2)\n",
    "                b3_ = src_.read(3)\n",
    "                b4_ = src_.read(4)\n",
    "                b5_ = src_.read(5)\n",
    "                b6_ = src_.read(6)\n",
    "                b7_ = src_.read(7)\n",
    "                b8_ = src_.read(8)\n",
    "                b9_ = src_.read(9)\n",
    "                b10_ = src_.read(10)\n",
    "                b11_ = src_.read(11)\n",
    "                b12_ = src_.read(12)\n",
    "                b13_ = src_.read(13)\n",
    "                b14_ = src_.read(14)\n",
    "            \n",
    "                b1_ = np.where(np.isinf(b1_), np.nan, b1_)\n",
    "                b2_ = np.where(np.isinf(b2_), np.nan, b2_)          \n",
    "                b3_ = np.where(np.isinf(b3_), np.nan, b3_)\n",
    "                b4_ = np.where(np.isinf(b4_), np.nan, b4_)\n",
    "                b5_ = np.where(np.isinf(b5_), np.nan, b5_)\n",
    "                b6_ = np.where(np.isinf(b6_), np.nan, b6_)\n",
    "                b7_ = np.where(np.isinf(b7_), np.nan, b7_)\n",
    "                b8_ = np.where(np.isinf(b8_), np.nan, b8_)\n",
    "                b9_ = np.where(np.isinf(b9_), np.nan, b9_)\n",
    "                b10_ = np.where(np.isinf(b10_), np.nan, b10_)\n",
    "                b11_ = np.where(np.isinf(b11_), np.nan, b11_)\n",
    "                b12_ = np.where(np.isinf(b12_), np.nan, b12_)\n",
    "                b13_ = np.where(np.isinf(b13_), np.nan, b13_)\n",
    "                b14_ = np.where(np.isinf(b14_), np.nan, b14_)\n",
    "                            \n",
    "                nan1 = 9\n",
    "                nan2 = 81\n",
    "                \n",
    "                b1_ = np.nan_to_num(b1_, nan=nan1)\n",
    "                b2_ = np.nan_to_num(b2_, nan=nan1)\n",
    "                b3_ = np.nan_to_num(b3_, nan=nan1)\n",
    "                b4_ = np.nan_to_num(b4_, nan=nan1)\n",
    "                b5_ = np.nan_to_num(b5_, nan=nan2)\n",
    "                b6_ = np.nan_to_num(b6_, nan=nan2)\n",
    "                b7_ = np.nan_to_num(b7_, nan=nan1)\n",
    "                b8_ = np.nan_to_num(b8_, nan=nan1)\n",
    "                b9_ = np.nan_to_num(b9_, nan=nan1)\n",
    "                b10_ = np.nan_to_num(b10_, nan=nan1)\n",
    "                b11_ = np.nan_to_num(b11_, nan=nan1)\n",
    "                b12_ = np.nan_to_num(b12_, nan=nan1)\n",
    "                b13_ = np.nan_to_num(b13_, nan=nan2)\n",
    "                b14_ = np.nan_to_num(b14_, nan=nan2)\n",
    "            \n",
    "            \n",
    "                mask1 = (b1_ ==9) & (b1_== 81)\n",
    "                mask2 = (b2_ ==9) & (b2_== 81)\n",
    "                mask3 = (b3_ ==9) & (b3_== 81)\n",
    "                mask4 = (b4_ ==9) & (b4_== 81)\n",
    "                mask5 = (b5_ ==9) & (b5_== 81)\n",
    "                mask6 = (b6_ ==9) & (b6_== 81)\n",
    "                mask7 = (b7_ ==9) & (b7_== 81)\n",
    "                mask8 = (b8_ ==9) & (b8_== 81)\n",
    "                mask9 = (b9_ ==9) & (b9_== 81)\n",
    "                mask10 = (b10_ ==9) & (b10_== 81)\n",
    "                mask11 = (b11_ ==9) & (b11_== 81)\n",
    "                mask12 = (b12_ ==9) & (b12_== 81)\n",
    "                mask13 = (b13_ ==9) & (b13_== 81)\n",
    "                mask14 = (b14_ ==9) & (b14_== 81)\n",
    "                \n",
    "                b1_ = np.ma.masked_array(b1_, mask1)\n",
    "                b2_ = np.ma.masked_array(b2_, mask2)\n",
    "                b3_ = np.ma.masked_array(b3_, mask3)\n",
    "                b4_ = np.ma.masked_array(b4_, mask4)\n",
    "                b5_ = np.ma.masked_array(b5_, mask5)\n",
    "                b6_ = np.ma.masked_array(b6_, mask6)\n",
    "                b7_ = np.ma.masked_array(b7_, mask7)\n",
    "                b8_ = np.ma.masked_array(b8_, mask8)\n",
    "                b9_ = np.ma.masked_array(b9_, mask9)\n",
    "                b10_ = np.ma.masked_array(b10_, mask10)\n",
    "                b11_ = np.ma.masked_array(b11_, mask11)\n",
    "                b12_ = np.ma.masked_array(b12_, mask12)\n",
    "                b13_ = np.ma.masked_array(b13_, mask13)\n",
    "                b14_ = np.ma.masked_array(b14_, mask14)\n",
    "            \n",
    "                # image_ = np.moveaxis(image_, 0, -1)  # Rearrange to (rows, cols, bands)\n",
    "            \n",
    "                bands_ = np.dstack((b1_,b2_,b3_,b4_,b5_,b6_,b7_,b8_,b9_,b10_,b11_,b12_,b13_,b14_))  # <-- Why must b1 be alpha? Odd.\n",
    "                bands_ = bands_.reshape(int(np.prod(bands_.shape)/14),14)\n",
    "            \n",
    "            \n",
    "            # Load the shapefile\n",
    "            shapefile_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/ROI/ROI_SUMMER/REMASTERx/HS_ice_water_MASTERx.shp\"\n",
    "            shapefile = gpd.read_file(shapefile_path)\n",
    "            \n",
    "            # Extract training data\n",
    "            # Assuming the shapefile has a 'class' column for labels\n",
    "            training_data = []\n",
    "            labels = []\n",
    "            \n",
    "            for _, row in shapefile.iterrows():\n",
    "                geom = row.geometry\n",
    "                label = row['Class']\n",
    "                mask = rio.features.geometry_mask([geom], transform=profile['transform'], invert=True, out_shape=image.shape[:2])\n",
    "                pixels = image[mask]\n",
    "                training_data.append(pixels)\n",
    "                labels.extend([label] * len(pixels))\n",
    "            \n",
    "            training_data = np.vstack(training_data)\n",
    "            labels = np.array(labels)\n",
    "            \n",
    "            #Train the SVM model\n",
    "            X_train, X_test, y_train, y_test = train_test_split(training_data, labels, test_size=0.3, random_state=42)\n",
    "            clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-3))\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            #Classify the image\n",
    "            predicted = clf.predict(bands_) #Predict using the model\n",
    "            class_image = predicted.reshape(b1_.shape)\n",
    "            \n",
    "            def image_to_geodataframe(image_array, transform, crs):\n",
    "            # Generate shapes (polygons) from the raster array\n",
    "                shapes_generator = shapes(image_array, transform=transform_)\n",
    "                \n",
    "                # Create a GeoDataFrame\n",
    "                geometries = []\n",
    "                values = []\n",
    "                for geom, value in shapes_generator:\n",
    "                    geometries.append(shape(geom))\n",
    "                    values.append(value)\n",
    "                \n",
    "                gdf = gpd.GeoDataFrame({'value': values, 'geometry': geometries}, crs=crs_)\n",
    "                \n",
    "                return gdf \n",
    "                \n",
    "            class_gdf = image_to_geodataframe(class_image, transform=transform_, crs=crs_)\n",
    "            \n",
    "            class_gdf['area'] = class_gdf.geometry.area\n",
    "            class_gdf['perimeter'] = class_gdf.geometry.length\n",
    "            class_gdf['centroid'] = class_gdf.geometry.centroid\n",
    "            # Extract latitude and longitude from centroids\n",
    "            class_gdf['longitude'] = class_gdf.centroid.x\n",
    "            class_gdf['latitude'] = class_gdf.centroid.y\n",
    "            class_gdf.set_crs(epsg=32633, inplace=True)\n",
    "            # print(class_gdf)\n",
    "            \n",
    "            # Function to calculate orientation\n",
    "            def calculate_orientation(geometry):\n",
    "                coords = np.array(geometry.exterior.coords)\n",
    "                x = coords[:, 0]\n",
    "                y = coords[:, 1]\n",
    "                \n",
    "                # Covariance matrix\n",
    "                cov_matrix = np.cov(x, y)\n",
    "                \n",
    "                # Eigenvalues and eigenvectors\n",
    "                eigvals, eigvecs = np.linalg.eig(cov_matrix)\n",
    "                \n",
    "                # Orientation angle (radians to degrees)\n",
    "                largest_eigenvec = eigvecs[:, np.argmax(eigvals)]\n",
    "                angle = np.arctan2(largest_eigenvec[1], largest_eigenvec[0])\n",
    "                return np.degrees(angle)\n",
    "            \n",
    "            # Function to calculate roundness\n",
    "            roundness = (4 * np.pi * class_gdf['area']) / (class_gdf['perimeter'] ** 2)\n",
    "            \n",
    "            # Apply orientation calculation\n",
    "            class_gdf[\"orientation\"] = class_gdf.geometry.apply(calculate_orientation)\n",
    "            class_gdf[\"roundness\"] = roundness\n",
    "            class_gdf.set_crs(epsg=32633, inplace=True)\n",
    "            gdf = class_gdf\n",
    "            # print(gdf)\n",
    "            \n",
    "            gdf.to_parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c58632-f280-44cf-acd0-cbe436395cf4",
   "metadata": {},
   "source": [
    "## Convert to dataframe for quadrants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c46305-715a-4874-be37-40311d5ae080",
   "metadata": {},
   "source": [
    "#### Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a40678-8723-42bd-9529-d63ea41e1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/S1/EW_parquet_2/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/S1/EW_Winter_reclass/\"\n",
    "\n",
    "raster_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/output_bandsSVMtrain_Pair.tif\"\n",
    "with rio.open(raster_path) as src_:\n",
    "    image_ = src_.read()  # Read all bands\n",
    "    profile_ = src_.profile  # Save metadata\n",
    "    transform_ = src_.transform\n",
    "    crs_= src_.crs\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all .tif files in the input directory\n",
    "for subdir, _, files in os.walk(input_dir):\n",
    "    for filename in files: \n",
    "        if filename.endswith(('.shp')):\n",
    "            input_path = os.path.join(subdir, filename)\n",
    "            output_path = os.path.join(output_dir, filename.replace(\".shp\", \".tif\"))\n",
    "              \n",
    "            with open(input_path) as file:\n",
    "                gdf = gpd.read_parquet(input_path)\n",
    "                # print(gdf.head())\n",
    "                \n",
    "                # Quadrants of the Fjord\n",
    "            \n",
    "                brepollen = gdf[(530000 <= gdf['longitude']) & (gdf['longitude'] <= 541628.7) & (gdf['latitude'] >= 8.542e+06) & (gdf['latitude'] <= 8.558e+06)] #533500\n",
    "                samarinvagen = gdf[(529000 <= gdf['longitude']) & (gdf['longitude'] <= 533000) & (gdf['latitude'] <= 8.545e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                burgerbukta = gdf[(519000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.549e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                isbjornhamna = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 517000) & (gdf['latitude'] >= 8.5470e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                greenlandsea = gdf[(498900 <= gdf['longitude']) & (gdf['longitude'] <= 513300)]\n",
    "                hornsund_quadrant1 = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 520000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5470e+06)]\n",
    "                hornsund_quadrant2 = gdf[(525000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5490e+06)]\n",
    "                hornsund_quadrant3 = gdf[(520000 <= gdf['longitude']) & (gdf['longitude'] <= 523400) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5440e+06)]\n",
    "                hornsund_quadrant4 = gdf[(528000 <= gdf['longitude']) & (gdf['longitude'] <= 530000) & (gdf['latitude'] >= 8.535e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                mainbasin = pd.concat([greenlandsea, hornsund_quadrant1, hornsund_quadrant2, hornsund_quadrant3, hornsund_quadrant4])\n",
    "                \n",
    "                \n",
    "                bpln_Nlent = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bpln_N_lent = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bplnlent = pd.concat([bpln_Nlent, bpln_N_lent])\n",
    "                \n",
    "                bpln_Nrd = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bpln_N_rd = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bplnrd = pd.concat([bpln_Nrd, bpln_N_rd])\n",
    "                bpln_fjord = pd.concat([bplnlent, bplnrd])\n",
    "                \n",
    "                samar_Nlent = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samar_N_lent = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samarlent = pd.concat([samar_Nlent, samar_N_lent])\n",
    "                \n",
    "                samar_Nrd = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samar_N_rd = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samarrd = pd.concat([samar_Nrd, samar_N_rd])\n",
    "                samar_fjord = pd.concat([samarlent, samarrd])\n",
    "                \n",
    "                burg_Nlent = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burg_N_lent = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burglent = pd.concat([burg_Nlent, burg_N_lent])\n",
    "                \n",
    "                burg_Nrd = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burg_N_rd = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burgrd = pd.concat([burg_Nrd, burg_N_rd])\n",
    "                burg_fjord = pd.concat([burglent, burgrd])\n",
    "                \n",
    "                isbj_Nlent = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbj_N_lent = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbjlent = pd.concat([isbj_Nlent, isbj_N_lent])\n",
    "                \n",
    "                isbj_Nrd = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbj_N_rd = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbjrd = pd.concat([isbj_Nrd, isbj_N_rd])\n",
    "                isbj_fjord = pd.concat([isbjlent,isbjrd])\n",
    "                \n",
    "                overlay = gdf[(gdf['area']<= 1000000000)]\n",
    "                \n",
    "                # Concatenate all to create a multipolygon shapefile\n",
    "                \n",
    "                clusterk = pd.concat([overlay,bplnlent,bplnrd,samarlent,samarrd,burglent,burgrd,isbjlent,isbjrd,mainbasin], \n",
    "                                     keys=['overlay','bplnlent','bplnrd','samarlent','samarrd','burglent','burgrd','isbjlent','isbjrd','mainbasin']).reset_index(level=0).rename(columns={'level_0': 'Source'})\n",
    "                \n",
    "                refined = clusterk[(clusterk['area'] >= 10000)]\n",
    "                \n",
    "                #Drawing the image to tif format\n",
    "                bounds = clusterk.total_bounds\n",
    "                resolution = 50  # Define resolution (e.g., 50x50 units per pixel)\n",
    "                width = int((bounds[2] - bounds[0]) / resolution)\n",
    "                height = int((bounds[3] - bounds[1]) / resolution)\n",
    "                transform = rio.transform.from_bounds(*bounds, width, height)\n",
    "                \n",
    "                # Define class colors (e.g., mapping vector attributes to raster values)\n",
    "                class_colors = {\n",
    "                    'overlay': 1,\n",
    "                    'bplnlent': 2,\n",
    "                    'bplnrd': 3,\n",
    "                    'samarlent': 4,\n",
    "                    'samarrd': 5,\n",
    "                    'burglent': 6,\n",
    "                    'burgrd': 7,\n",
    "                    'isbjlent': 8,\n",
    "                    'isbjrd': 9,\n",
    "                    'mainbasin': 10,\n",
    "                }\n",
    "                # Prepare shapes and values for rasterization\n",
    "                shapes = [(geom, class_colors[attr]) for geom, attr in zip(refined.geometry, refined['Source'])]\n",
    "                \n",
    "                # Rasterize vector data\n",
    "                rasterized = rasterize(\n",
    "                    shapes=shapes,\n",
    "                    out_shape=(height, width),\n",
    "                    transform=transform_,\n",
    "                    fill=0,  # Background value\n",
    "                    dtype=\"uint8\" \n",
    "                )\n",
    "                # plt.imshow(rasterized)\n",
    "                \n",
    "                # plt.savefig(output_path, dpi=1500,     #resoluton  \n",
    "                #            bbox_inches='tight',  # Tight layout\n",
    "                #            pad_inches=0.1,       # Padding\n",
    "                #            facecolor='white',    # Background color\n",
    "                #            edgecolor='none',     # Edge color\n",
    "                #            format='png')\n",
    "                with rio.open(output_path, \"w\", driver=\"GTiff\", height=height, width=width, count=1, dtype=rasterized.dtype, crs= src_.crs, transform=transform_,) as dst:\n",
    "                    dst.write(rasterized, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f314e-f7cc-442d-81da-f8ee675860f0",
   "metadata": {},
   "source": [
    "#### Summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad41643-ea51-4aa3-8eab-76040c8bda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/multi_class_summer/summer_parquet/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/multi_class_summer/Summer_reclass\"\n",
    "\n",
    "raster_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/output_bandsSVMtrain_Pair.tif\"\n",
    "with rio.open(raster_path) as src_:\n",
    "    image_ = src_.read()  # Read all bands\n",
    "    profile_ = src_.profile  # Save metadata\n",
    "    transform_ = src_.transform\n",
    "    crs_= src_.crs\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all .tif files in the input directory\n",
    "for subdir, _, files in os.walk(input_dir):\n",
    "    for filename in files: \n",
    "        if filename.endswith(('.shp')):\n",
    "            input_path = os.path.join(subdir, filename)\n",
    "            output_path = os.path.join(output_dir, filename.replace(\".shp\", \".tif\"))\n",
    "              \n",
    "            with open(input_path) as file:\n",
    "                gdf = gpd.read_parquet(input_path)\n",
    "                # print(gdf.head())\n",
    "                \n",
    "                # Quadrants of the Fjord\n",
    "            \n",
    "                brepollen = gdf[(530000 <= gdf['longitude']) & (gdf['longitude'] <= 541628.7) & (gdf['latitude'] >= 8.542e+06) & (gdf['latitude'] <= 8.558e+06)] #533500\n",
    "                samarinvagen = gdf[(529000 <= gdf['longitude']) & (gdf['longitude'] <= 533000) & (gdf['latitude'] <= 8.545e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                burgerbukta = gdf[(519000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.549e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                isbjornhamna = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 517000) & (gdf['latitude'] >= 8.5470e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                greenlandsea = gdf[(498900 <= gdf['longitude']) & (gdf['longitude'] <= 513300)]\n",
    "                hornsund_quadrant1 = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 520000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5470e+06)]\n",
    "                hornsund_quadrant2 = gdf[(525000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5490e+06)]\n",
    "                hornsund_quadrant3 = gdf[(520000 <= gdf['longitude']) & (gdf['longitude'] <= 523400) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5440e+06)]\n",
    "                hornsund_quadrant4 = gdf[(528000 <= gdf['longitude']) & (gdf['longitude'] <= 530000) & (gdf['latitude'] >= 8.535e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                mainbasin = pd.concat([greenlandsea, hornsund_quadrant1, hornsund_quadrant2, hornsund_quadrant3, hornsund_quadrant4])\n",
    "                \n",
    "                \n",
    "                bpln_Nlent = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bpln_N_lent = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bplnlent = pd.concat([bpln_Nlent, bpln_N_lent])\n",
    "                \n",
    "                bpln_Nrd = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bpln_N_rd = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bplnrd = pd.concat([bpln_Nrd, bpln_N_rd])\n",
    "                bpln_fjord = pd.concat([bplnlent, bplnrd])\n",
    "                \n",
    "                samar_Nlent = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samar_N_lent = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samarlent = pd.concat([samar_Nlent, samar_N_lent])\n",
    "                \n",
    "                samar_Nrd = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samar_N_rd = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samarrd = pd.concat([samar_Nrd, samar_N_rd])\n",
    "                samar_fjord = pd.concat([samarlent, samarrd])\n",
    "                \n",
    "                burg_Nlent = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burg_N_lent = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burglent = pd.concat([burg_Nlent, burg_N_lent])\n",
    "                \n",
    "                burg_Nrd = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burg_N_rd = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burgrd = pd.concat([burg_Nrd, burg_N_rd])\n",
    "                burg_fjord = pd.concat([burglent, burgrd])\n",
    "                \n",
    "                isbj_Nlent = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbj_N_lent = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbjlent = pd.concat([isbj_Nlent, isbj_N_lent])\n",
    "                \n",
    "                isbj_Nrd = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbj_N_rd = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbjrd = pd.concat([isbj_Nrd, isbj_N_rd])\n",
    "                isbj_fjord = pd.concat([isbjlent,isbjrd])\n",
    "                \n",
    "                overlay = gdf[(gdf['area']<= 1000000000)]\n",
    "                \n",
    "                # Concatenate all to create a multipolygon shapefile\n",
    "                \n",
    "                clusterk = pd.concat([overlay,bplnlent,bplnrd,samarlent,samarrd,burglent,burgrd,isbjlent,isbjrd,mainbasin], \n",
    "                                     keys=['overlay','bplnlent','bplnrd','samarlent','samarrd','burglent','burgrd','isbjlent','isbjrd','mainbasin']).reset_index(level=0).rename(columns={'level_0': 'Source'})\n",
    "                \n",
    "                refined = clusterk[(clusterk['area'] >= 10000)]\n",
    "                \n",
    "                #Drawing the image to tif format\n",
    "                bounds = clusterk.total_bounds\n",
    "                resolution = 50  # Define resolution (e.g., 50x50 units per pixel)\n",
    "                width = int((bounds[2] - bounds[0]) / resolution)\n",
    "                height = int((bounds[3] - bounds[1]) / resolution)\n",
    "                transform = rio.transform.from_bounds(*bounds, width, height)\n",
    "                \n",
    "                # Define class colors (e.g., mapping vector attributes to raster values)\n",
    "                class_colors = {\n",
    "                    'overlay': 1,\n",
    "                    'bplnlent': 2,\n",
    "                    'bplnrd': 3,\n",
    "                    'samarlent': 4,\n",
    "                    'samarrd': 5,\n",
    "                    'burglent': 6,\n",
    "                    'burgrd': 7,\n",
    "                    'isbjlent': 8,\n",
    "                    'isbjrd': 9,\n",
    "                    'mainbasin': 10,\n",
    "                }\n",
    "                # Prepare shapes and values for rasterization\n",
    "                shapes = [(geom, class_colors[attr]) for geom, attr in zip(refined.geometry, refined['Source'])]\n",
    "                \n",
    "                # Rasterize vector data\n",
    "                rasterized = rasterize(\n",
    "                    shapes=shapes,\n",
    "                    out_shape=(height, width),\n",
    "                    transform=transform_,\n",
    "                    fill=0,  # Background value\n",
    "                    dtype=\"uint8\" \n",
    "                )\n",
    "                # plt.imshow(rasterized)\n",
    "                \n",
    "                # plt.savefig(output_path, dpi=1500,     #resoluton  \n",
    "                #            bbox_inches='tight',  # Tight layout\n",
    "                #            pad_inches=0.1,       # Padding\n",
    "                #            facecolor='white',    # Background color\n",
    "                #            edgecolor='none',     # Edge color\n",
    "                #            format='png')\n",
    "                with rio.open(output_path, \"w\", driver=\"GTiff\", height=height, width=width, count=1, dtype=rasterized.dtype, crs= src_.crs, transform=transform_,) as dst:\n",
    "                    dst.write(rasterized, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a097d16-661b-400a-a007-0eb1a9f35126",
   "metadata": {},
   "source": [
    "## Binary reclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f66de-2074-46c4-917d-02cf87cc9505",
   "metadata": {},
   "source": [
    "#### Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10502653-531a-494f-b74a-eef0decd6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/S1/EW_parquet_2/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/S1/EW_Winter_reclass_binary/\"\n",
    "\n",
    "raster_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/output_bandsSVMtrain_Pair.tif\"\n",
    "with rio.open(raster_path) as src_:\n",
    "    image_ = src_.read()  # Read all bands\n",
    "    profile_ = src_.profile  # Save metadata\n",
    "    transform_ = src_.transform\n",
    "    crs_= src_.crs\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all .tif files in the input directory\n",
    "for subdir, _, files in os.walk(input_dir):\n",
    "    for filename in files: \n",
    "        if filename.endswith(('.shp')):\n",
    "            input_path = os.path.join(subdir, filename)\n",
    "            output_path = os.path.join(output_dir, filename.replace(\".shp\", \".tif\"))\n",
    "              \n",
    "            with open(input_path) as file:\n",
    "                gdf = gpd.read_parquet(input_path)\n",
    "                # print(gdf.head())\n",
    "                \n",
    "                # Quadrants of the Fjord\n",
    "            \n",
    "                brepollen = gdf[(530000 <= gdf['longitude']) & (gdf['longitude'] <= 541628.7) & (gdf['latitude'] >= 8.542e+06) & (gdf['latitude'] <= 8.558e+06)] #533500\n",
    "                samarinvagen = gdf[(529000 <= gdf['longitude']) & (gdf['longitude'] <= 533000) & (gdf['latitude'] <= 8.545e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                burgerbukta = gdf[(519000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.549e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                isbjornhamna = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 517000) & (gdf['latitude'] >= 8.5470e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                greenlandsea = gdf[(498900 <= gdf['longitude']) & (gdf['longitude'] <= 513300)]\n",
    "                hornsund_quadrant1 = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 520000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5470e+06)]\n",
    "                hornsund_quadrant2 = gdf[(525000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5490e+06)]\n",
    "                hornsund_quadrant3 = gdf[(520000 <= gdf['longitude']) & (gdf['longitude'] <= 523400) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5440e+06)]\n",
    "                hornsund_quadrant4 = gdf[(528000 <= gdf['longitude']) & (gdf['longitude'] <= 530000) & (gdf['latitude'] >= 8.535e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                mainbasin = pd.concat([greenlandsea, hornsund_quadrant1, hornsund_quadrant2, hornsund_quadrant3, hornsund_quadrant4])\n",
    "                \n",
    "                \n",
    "                bpln_Nlent = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bpln_N_lent = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bplnlent = pd.concat([bpln_Nlent, bpln_N_lent])\n",
    "                \n",
    "                bpln_Nrd = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bpln_N_rd = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bplnrd = pd.concat([bpln_Nrd, bpln_N_rd])\n",
    "                bpln_fjord = pd.concat([bplnlent, bplnrd])\n",
    "                \n",
    "                samar_Nlent = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samar_N_lent = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samarlent = pd.concat([samar_Nlent, samar_N_lent])\n",
    "                \n",
    "                samar_Nrd = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samar_N_rd = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samarrd = pd.concat([samar_Nrd, samar_N_rd])\n",
    "                samar_fjord = pd.concat([samarlent, samarrd])\n",
    "                \n",
    "                burg_Nlent = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burg_N_lent = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burglent = pd.concat([burg_Nlent, burg_N_lent])\n",
    "                \n",
    "                burg_Nrd = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burg_N_rd = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burgrd = pd.concat([burg_Nrd, burg_N_rd])\n",
    "                burg_fjord = pd.concat([burglent, burgrd])\n",
    "                \n",
    "                isbj_Nlent = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbj_N_lent = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbjlent = pd.concat([isbj_Nlent, isbj_N_lent])\n",
    "                \n",
    "                isbj_Nrd = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbj_N_rd = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbjrd = pd.concat([isbj_Nrd, isbj_N_rd])\n",
    "                isbj_fjord = pd.concat([isbjlent,isbjrd])\n",
    "                \n",
    "                overlay = gdf[(gdf['area']<= 1000000000)]\n",
    "                \n",
    "                # Concatenate all to create a multipolygon shapefile\n",
    "                \n",
    "                clusterk = pd.concat([overlay,bplnlent,bplnrd,samarlent,samarrd,burglent,burgrd,isbjlent,isbjrd,mainbasin], \n",
    "                                     keys=['overlay','bplnlent','bplnrd','samarlent','samarrd','burglent','burgrd','isbjlent','isbjrd','mainbasin']).reset_index(level=0).rename(columns={'level_0': 'Source'})\n",
    "                \n",
    "                refined = clusterk[(clusterk['area'] >= 10000)]\n",
    "                \n",
    "                #Drawing the image to tif format\n",
    "                bounds = clusterk.total_bounds\n",
    "                resolution = 50  # Define resolution (e.g., 50x50 units per pixel)\n",
    "                width = int((bounds[2] - bounds[0]) / resolution)\n",
    "                height = int((bounds[3] - bounds[1]) / resolution)\n",
    "                transform = rio.transform.from_bounds(*bounds, width, height)\n",
    "                \n",
    "                # Define class colors (e.g., mapping vector attributes to raster values)\n",
    "                class_colors = {\n",
    "                    'overlay': 0,\n",
    "                    'bplnlent': 1,\n",
    "                    'bplnrd': 1,\n",
    "                    'samarlent': 1,\n",
    "                    'samarrd': 1,\n",
    "                    'burglent': 1,\n",
    "                    'burgrd': 1,\n",
    "                    'isbjlent': 1,\n",
    "                    'isbjrd': 1,\n",
    "                    'mainbasin': 1,\n",
    "                }\n",
    "                # Prepare shapes and values for rasterization\n",
    "                shapes = [(geom, class_colors[attr]) for geom, attr in zip(refined.geometry, refined['Source'])]\n",
    "                \n",
    "                # Rasterize vector data\n",
    "                rasterized = rasterize(\n",
    "                    shapes=shapes,\n",
    "                    out_shape=(height, width),\n",
    "                    transform=transform_,\n",
    "                    fill=0,  # Background value\n",
    "                    dtype=\"uint8\" \n",
    "                )\n",
    "                # plt.imshow(rasterized)\n",
    "                \n",
    "                # plt.savefig(output_path, dpi=1500,     #resoluton  \n",
    "                #            bbox_inches='tight',  # Tight layout\n",
    "                #            pad_inches=0.1,       # Padding\n",
    "                #            facecolor='white',    # Background color\n",
    "                #            edgecolor='none',     # Edge color\n",
    "                #            format='png')\n",
    "                with rio.open(output_path, \"w\", driver=\"GTiff\", height=height, width=width, count=1, dtype=rasterized.dtype, crs= src_.crs, transform=transform_,) as dst:\n",
    "                    dst.write(rasterized, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a821014e-ba75-4966-a6cc-45a7af03406b",
   "metadata": {},
   "source": [
    "#### Summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b8197f-d300-45ab-af85-7a8290002f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/multi_class_summer/summer_parquet/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/multi_class_summer/Summer_reclass_binary/\"\n",
    "\n",
    "raster_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/output_bandsSVMtrain_Pair.tif\"\n",
    "with rio.open(raster_path) as src_:\n",
    "    image_ = src_.read()  # Read all bands\n",
    "    profile_ = src_.profile  # Save metadata\n",
    "    transform_ = src_.transform\n",
    "    crs_= src_.crs\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all .tif files in the input directory\n",
    "for subdir, _, files in os.walk(input_dir):\n",
    "    for filename in files: \n",
    "        if filename.endswith(('.shp')):\n",
    "            input_path = os.path.join(subdir, filename)\n",
    "            output_path = os.path.join(output_dir, filename.replace(\".shp\", \".tif\"))\n",
    "              \n",
    "            with open(input_path) as file:\n",
    "                gdf = gpd.read_parquet(input_path)\n",
    "                # print(gdf.head())\n",
    "                \n",
    "                # Quadrants of the Fjord\n",
    "            \n",
    "                brepollen = gdf[(530000 <= gdf['longitude']) & (gdf['longitude'] <= 541628.7) & (gdf['latitude'] >= 8.542e+06) & (gdf['latitude'] <= 8.558e+06)] #533500\n",
    "                samarinvagen = gdf[(529000 <= gdf['longitude']) & (gdf['longitude'] <= 533000) & (gdf['latitude'] <= 8.545e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                burgerbukta = gdf[(519000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.549e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                isbjornhamna = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 517000) & (gdf['latitude'] >= 8.5470e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                greenlandsea = gdf[(498900 <= gdf['longitude']) & (gdf['longitude'] <= 513300)]\n",
    "                hornsund_quadrant1 = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 520000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5470e+06)]\n",
    "                hornsund_quadrant2 = gdf[(525000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5490e+06)]\n",
    "                hornsund_quadrant3 = gdf[(520000 <= gdf['longitude']) & (gdf['longitude'] <= 523400) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5440e+06)]\n",
    "                hornsund_quadrant4 = gdf[(528000 <= gdf['longitude']) & (gdf['longitude'] <= 530000) & (gdf['latitude'] >= 8.535e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                mainbasin = pd.concat([greenlandsea, hornsund_quadrant1, hornsund_quadrant2, hornsund_quadrant3, hornsund_quadrant4])\n",
    "                \n",
    "                \n",
    "                bpln_Nlent = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bpln_N_lent = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bplnlent = pd.concat([bpln_Nlent, bpln_N_lent])\n",
    "                \n",
    "                bpln_Nrd = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bpln_N_rd = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bplnrd = pd.concat([bpln_Nrd, bpln_N_rd])\n",
    "                bpln_fjord = pd.concat([bplnlent, bplnrd])\n",
    "                \n",
    "                samar_Nlent = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samar_N_lent = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samarlent = pd.concat([samar_Nlent, samar_N_lent])\n",
    "                \n",
    "                samar_Nrd = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samar_N_rd = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samarrd = pd.concat([samar_Nrd, samar_N_rd])\n",
    "                samar_fjord = pd.concat([samarlent, samarrd])\n",
    "                \n",
    "                burg_Nlent = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burg_N_lent = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burglent = pd.concat([burg_Nlent, burg_N_lent])\n",
    "                \n",
    "                burg_Nrd = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burg_N_rd = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burgrd = pd.concat([burg_Nrd, burg_N_rd])\n",
    "                burg_fjord = pd.concat([burglent, burgrd])\n",
    "                \n",
    "                isbj_Nlent = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbj_N_lent = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbjlent = pd.concat([isbj_Nlent, isbj_N_lent])\n",
    "                \n",
    "                isbj_Nrd = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbj_N_rd = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbjrd = pd.concat([isbj_Nrd, isbj_N_rd])\n",
    "                isbj_fjord = pd.concat([isbjlent,isbjrd])\n",
    "                \n",
    "                overlay = gdf[(gdf['area']<= 1000000000)]\n",
    "                \n",
    "                # Concatenate all to create a multipolygon shapefile\n",
    "                \n",
    "                clusterk = pd.concat([overlay,bplnlent,bplnrd,samarlent,samarrd,burglent,burgrd,isbjlent,isbjrd,mainbasin], \n",
    "                                     keys=['overlay','bplnlent','bplnrd','samarlent','samarrd','burglent','burgrd','isbjlent','isbjrd','mainbasin']).reset_index(level=0).rename(columns={'level_0': 'Source'})\n",
    "                \n",
    "                refined = clusterk[(clusterk['area'] >= 10000)]\n",
    "                \n",
    "                #Drawing the image to tif format\n",
    "                bounds = clusterk.total_bounds\n",
    "                resolution = 50  # Define resolution (e.g., 50x50 units per pixel)\n",
    "                width = int((bounds[2] - bounds[0]) / resolution)\n",
    "                height = int((bounds[3] - bounds[1]) / resolution)\n",
    "                transform = rio.transform.from_bounds(*bounds, width, height)\n",
    "                \n",
    "                # Define class colors (e.g., mapping vector attributes to raster values)\n",
    "                class_colors = {\n",
    "                    'overlay': 0,\n",
    "                    'bplnlent': 1,\n",
    "                    'bplnrd': 1,\n",
    "                    'samarlent': 1,\n",
    "                    'samarrd': 1,\n",
    "                    'burglent': 1,\n",
    "                    'burgrd': 1,\n",
    "                    'isbjlent': 1,\n",
    "                    'isbjrd': 1,\n",
    "                    'mainbasin': 1,\n",
    "                }\n",
    "                # Prepare shapes and values for rasterization\n",
    "                shapes = [(geom, class_colors[attr]) for geom, attr in zip(refined.geometry, refined['Source'])]\n",
    "                \n",
    "                # Rasterize vector data\n",
    "                rasterized = rasterize(\n",
    "                    shapes=shapes,\n",
    "                    out_shape=(height, width),\n",
    "                    transform=transform_,\n",
    "                    fill=0,  # Background value\n",
    "                    dtype=\"uint8\" \n",
    "                )\n",
    "                # plt.imshow(rasterized)\n",
    "                \n",
    "                # plt.savefig(output_path, dpi=1500,     #resoluton  \n",
    "                #            bbox_inches='tight',  # Tight layout\n",
    "                #            pad_inches=0.1,       # Padding\n",
    "                #            facecolor='white',    # Background color\n",
    "                #            edgecolor='none',     # Edge color\n",
    "                #            format='png')\n",
    "                with rio.open(output_path, \"w\", driver=\"GTiff\", height=height, width=width, count=1, dtype=rasterized.dtype, crs= src_.crs, transform=transform_,) as dst:\n",
    "                    dst.write(rasterized, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c23947-4646-4279-81a6-28ba32f331f6",
   "metadata": {},
   "source": [
    "## Remove land area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e55b1c72-2bba-416c-9a42-105762c90dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped image saved to C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/multi_class_summer/2015_2016/RS2_SCWA_20160301_055340_DES_338_einc_abb73a4d_8fdf_4f75_85e3_d5c51622ea61.tif\n"
     ]
    }
   ],
   "source": [
    "# Clip image and maintain georeferencing\n",
    "\n",
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/multi_class_summer/WRB/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/multi_class_summer/2015_2016/\"\n",
    "shapefile_path = 'C:/Users/jullian.williams/Desktop/HIRLOMAP/hornsund_zmail/vector/2015.shp'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all .tif files in the input directory\n",
    "for subdir, _, files in os.walk(input_dir):\n",
    "    for filename in files: \n",
    "        # Check if the file is an image (e.g., .jpg, .png)\n",
    "        if filename.endswith(('.tif')):\n",
    "            input_path = os.path.join(subdir, filename)\n",
    "            output_path = os.path.join(output_dir, filename.replace(\".tif\", \".tif\"))\n",
    "\n",
    "\n",
    "            # Load the shapefile for clipping\n",
    "            shapefile = gpd.read_file(shapefile_path)\n",
    "            geometries = [feature[\"geometry\"] for feature in shapefile.__geo_interface__[\"features\"]]\n",
    "        \n",
    "            with rio.open(input_path) as src:\n",
    "            # Step 3: Clip the raster\n",
    "                clipped_image, clipped_transform = mask(src, geometries, crop=True)\n",
    "        \n",
    "                # Save the clipped image\n",
    "                clipped_meta = src.meta.copy()\n",
    "                clipped_meta.update({\n",
    "                    \"driver\": \"GTiff\",\n",
    "                    \"height\": clipped_image.shape[1],\n",
    "                    \"width\": clipped_image.shape[2],\n",
    "                    \"transform\": clipped_transform\n",
    "                })\n",
    "\n",
    "                \n",
    "                with rio.open(output_path, \"w\", **clipped_meta) as dst:\n",
    "                    dst.write(clipped_image)\n",
    "                \n",
    "print(f\"Clipped image saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
